VMware Tanzu Application platform also provides several tools for the runtime of the application.

##### Provisioning and consumption of backing services

VMware Tanzu Application Platform makes it easy as possible to discover, curate, consume, and manage backing services, such as databases, queues, and caches, across single or multi-cluster environments. 

This experience is made possible by using the **Services Toolkit** component. 

The **order service** uses a **PostgreSQL database** to store orders and **RabbitMQ** to asynchronously communicate with the **shipping service**.

**TODO: Have a closer look at the oder-service implementation**

For the best experience, developers should have a self-service to provision backing services across all the stages for the application.

The Services Toolkit provides **dynamic provisioning** capabilities with **Crossplane**, and we are also **partnering with the company behind that open-source project called Upbound**.
Support for additional provisioners might be added in the future.

In this workshop, we will run commands manually to dynamically provision a backing services for our application - in production environments, this should be automated.

Let's first discover available service classes in the cluster. 
```terminal:execute
command: tanzu service class list
clear: true
```

There are several [Bitnami](https://bitnami.com) Helm charts for data services available, which are pre-installed with TAP as examples. Those services can run everywhere where Crossplane supports the provisioning, e.g. in a Kubernetes cluster or native on a public cloud.

The service classes abstraction and Crossplane make it also possible to have a different way of provisioning for e.g. a PostgreSQL database in different stage without changing the workload or resources generated by the supply chain for it. So a PostgreSQL database in test cluster could be dynamically provisioned as Helm Chart, and for production as a native AWS service. 

We can have a closer look at a service class to see available configuration options exposed by the platform operators.
```terminal:execute
command: tanzu service class get postgresql-unmanaged
clear: true
```
Now we can claim the pre-installed Bitnami PostgreSQL service to obtain such a database.
```terminal:execute
command: tanzu service class-claim create postgres-1 --class postgresql-unmanaged --parameter storageGB=0.5
clear: true
```
It might take a moment or two before the claim reports `Ready: True`. After the claim is ready, you then have a successful claim for a PostgreSQL database.
```terminal:execute
command: tanzu services class-claims get postgres-1
clear: true
```
Let's also trigger the provisioning of a RabbitMq instance.
```terminal:execute
command: tanzu service class-claim create rmq-1 --class rabbitmq-unmanaged --parameter storageGB=0.5
clear: true
```

Services Toolkit also provides the **functionality to automatically inject credentials that are required for the connection to the backing services** into the containers of the running application via the [Service Binding Specification](https://github.com/k8s-service-bindings/spec) for Kubernetes. 

The developer defines the backing services the application wants to bind to in the Workload. Those backing services have to be available in the cluster or registered in it to be consumed by the service binding.

**TODO: Interactively add Service Bindings to the already available workload.yaml**

**TODO: Apply the updated workload and may add code to the app that it run with an in-memory db by default - Show it with App life view environment**

##### Application Live View

**TODO**

##### Serverless runtime

After we've finally had a look at all the different steps of our supply chain let's open the provided URL to access the order service.
```terminal:execute
command: tanzu apps workload get order-service
clear: true
```

The URL is also available in the detail view of the Delivery step in TAP-GUI.
Or you can just call it here:
```dashboard:open-url
url: https://order-service-{{ session_namespace }}.{{ ENV_TAP_INGRESS }}
```

When you open the URL in the browser, you may see that TLS is configured, and it takes some time until you get a first response from the application.

Both are, in a way, features of our commercial **Cloud Native Runtimes for VMware Tanzu** (CNRs), which is a serverless application runtime for Kubernetes that is based on **Knative**.

Knative, is an open-source community project, which provides a simple, consistent layer over Kubernetes that solves common problems of deploying software, connecting disparate systems together, upgrading software, observing software, routing traffic, and scaling automatically. 
```dashboard:open-url
url: https://knative.dev/docs/
```

The major **subprojects of Knative** are Serving and Eventing.
- **Serving** supports deploying upgrading, routing, and scaling of stateless applications and functions 
- **Eventing** enables developers to use an event-driven architecture with serverless applications and is **out of the scope of this workshop**
- **Functions**: Enables developers to easily create, build, and deploy stateless, event-driven functions

**Knative Serving abstracts away a lot of those resources** we usually have to configure to get an application running, like a deployment, service, ingress etc.

It also provides **configurable auto-scaling** and **scale to zero**, which is the reason why you had to wait for some seconds after you first called your application. Other features are rollbacks, canary and blue-green deployment via revisions, and traffic splitting.

More runtime components of VMware Tanzu Application Platform will be covered in the workshop.
Let's now focus on the challenges of our typical microservice application and how to mitigate them. 